# Auxiliary model/training configuration
# Copyright (c) 2021 Idiap Research Institute, http://www.idiap.ch/
# Written by Parvaneh Janbakhshi <parvaneh.janbakhshi@idiap.ch>

SelectedAuxiliaryNetwork: "MLP"
SelectedEncodedLayers: -1                                 # list of indices of MLP layers or -1. If it is a list, it concatenates the output of layers with indices in the list
                                                          # otherwise (-1) last layer of MLP encoder is used.

MLP:                                                      # Dropout > Linear layer > nonlinearity > Linear layer > nonlinearity ... > Linear layer > outputdim
  outputdim: 45                                            # number of classes--> output size
  hidden_units: [64, 64]                                  # Number of layers and their hidden units before output. if is [] means 1 layer for connecting input and output
  nonlinearity: "leaky-relu"                              # The non-linear activation function "leaky-relu", "relu" or no nonlinearity ""
  dropout_prob: 0.2                                       # The dropout probability after encoded layer (before MLP)

dataloader:
  online: False                                           # If True it computes feature-on-the-fly (online), otherwise uses saved features
  num_workers: 4                                          # torch Dataloader workers
  batch_size: 128                                         # batch size
  sequence_length: 500                                    # miliseconds (--> frames) segmenting audio as inputs for networks (both online and offline)
  data_path: preprocess/pc_gita_vowels_augmented_processed/folds_spkID_task/ # Source data path, 'in folder preprocess/Dataset_name/folds_spkID_task
  fs: 16000                                               # sampling frequency (needed for online feature extraction)

                                                          # Training options
runner:
 # the following commented options are dictated by the main upstream config file
  # Max_epoch: 100                                        # total steps for training
  optimizer:
    # type: 'Adam'                                        # optimizer type: ['Adam', 'SGD'].
    # lr: 1e-3                                            # Learning rate for downstream opt.
    # minlr: 1e-7                                         # minimum (lower bound) lr after decreasing learning rate for early stopping
    auxlr_ratio: 1                                        # ratio of learning rate for auxiliary training, i.e., aux_lr = upslr_ratio * lr
    loss: "CE"                                            # loss: MSE or CE (Cross Entropy)

